{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d3fab5d21ae3b3992984b238a67f5b0d95b393a"
   },
   "source": [
    "# Clean Manual Feature Engineering\n",
    "\n",
    "The purpose of this notebook is to clean up the manual feature engineering I had scattered over several other kernels. We will implement the complete manual feature engineering and then test the results.\n",
    "\n",
    "Update July 25: __This still cannot complete in the kernels.__ I took the script and ran it on a personal computer. The features themselves are available at https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features under `clean_manual.csv`. The feature importances for these features in a gradient boosting model are also available at the same link with the name `fi_clen\n",
    "\n",
    "### Roadmap\n",
    "\n",
    "Our plan of action is as follows.We have to be very careful about memory usage in the kernels, which affects the order of operations:\n",
    "\n",
    "1. Define functions:\n",
    "    * `agg_numeric`\n",
    "    * `agg_categorical`\n",
    "    * `agg_child` \n",
    "    * `agg_grandchild`\n",
    " 2. Add in domain knowledge features to `app`\n",
    " 3. Work through the `bureau` and `bureau_balance` data\n",
    "     * Add in hand built features\n",
    "     * Aggregate both using the appropriate functions\n",
    "     * Merge with `app` and delete the dataframes\n",
    "4. Work through `previous`, `installments`, `cash`, and `credit`\n",
    "    * Add in hand built features\n",
    "    * Aggregate using the appropriate functions\n",
    "    * Merge with `app` and delete the dataframes\n",
    "5. Modeling using a Gradient Boosting Machine\n",
    "    * Train model on training data using best hyperparameters from random search notebook\n",
    "    * Make predictions and submit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the datasets and replace the anomalous values\n",
    "app_train = pd.read_csv('../input/application_train.csv').replace({365243: np.nan})\n",
    "app_test = pd.read_csv('../input/application_test.csv').replace({365243: np.nan})\n",
    "bureau = pd.read_csv('../input/bureau.csv').replace({365243: np.nan})\n",
    "bureau_balance = pd.read_csv('../input/bureau_balance.csv').replace({365243: np.nan})\n",
    "\n",
    "app_test['TARGET'] = np.nan\n",
    "app = app_train.append(app_test, ignore_index = True, sort = True)\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "del app_train, app_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce614aaf0bf38ad124fc998807953150442eb0b6"
   },
   "source": [
    "# Numeric Aggregation Function\n",
    "\n",
    "The following function aggregates all the numeric variables in a child dataframe at the parent level. That is, for each parent, gather together (group) all of their children, and calculate the aggregations statistics across the children. The function also removes any columns that share the exact same values (which might happen using `count`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f474ba5bc0ea8aabf811ba31a59caa07625264bc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agg_numeric(df, parent_var, df_name):\n",
    "    \"\"\"\n",
    "    Groups and aggregates the numeric values in a child dataframe\n",
    "    by the parent variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df (dataframe): \n",
    "            the child dataframe to calculate the statistics on\n",
    "        parent_var (string): \n",
    "            the parent variable used for grouping and aggregating\n",
    "        df_name (string): \n",
    "            the variable used to rename the columns\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe): \n",
    "            a dataframe with the statistics aggregated by the `parent_var` for \n",
    "            all numeric columns. Each observation of the parent variable will have \n",
    "            one row in the dataframe with the parent variable as the index. \n",
    "            The columns are also renamed using the `df_name`. Columns with all duplicate\n",
    "            values are removed. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in df:\n",
    "        if col != parent_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns = col)\n",
    "            \n",
    "    # Only want the numeric variables\n",
    "    parent_ids = df[parent_var].copy()\n",
    "    numeric_df = df.select_dtypes('number').copy()\n",
    "    numeric_df[parent_var] = parent_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = []\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        if var != parent_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    agg.columns = columns\n",
    "    \n",
    "    # Remove the columns with all redundant values\n",
    "    _, idx = np.unique(agg, axis = 1, return_index=True)\n",
    "    agg = agg.iloc[:, idx]\n",
    "    \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc017cbe13c09e1651ef18a73066ac4bec2e1c59"
   },
   "source": [
    "# Categorical Aggregation Function\n",
    "\n",
    "Much like the numerical aggregation function, the `agg_categorical` function works on a child dataframe to aggregate statistics at the parent level. This can work with any child of `app` and might even be extensible to other problems with only minor changes in syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d848e33f0e04383c378798f02025344be5e7349b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agg_categorical(df, parent_var, df_name):\n",
    "    \"\"\"\n",
    "    Aggregates the categorical features in a child dataframe\n",
    "    for each observation of the parent variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe \n",
    "        The dataframe to calculate the value counts for.\n",
    "        \n",
    "    parent_var : string\n",
    "        The variable by which to group and aggregate the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "        \n",
    "    df_name : string\n",
    "        Variable added to the front of column names to keep track of columns\n",
    "\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with aggregated statistics for each observation of the parent_var\n",
    "        The columns are also renamed and columns with duplicate values are removed.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the categorical columns\n",
    "    categorical = pd.get_dummies(df.select_dtypes('object'))\n",
    "\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[parent_var] = df[parent_var]\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean'])\n",
    "    \n",
    "    column_names = []\n",
    "    \n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['sum', 'count', 'mean']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    categorical.columns = column_names\n",
    "    \n",
    "    # Remove duplicate columns by values\n",
    "    _, idx = np.unique(categorical, axis = 1, return_index = True)\n",
    "    categorical = categorical.iloc[:, idx]\n",
    "    \n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec2d17e34ca09c57038be703e48de7fbe1a7272b"
   },
   "source": [
    "# Combined Aggregation Function\n",
    "\n",
    "We can put these steps together into a function that will handle a child dataframe. The function will take care of both the numeric and categorical variables and will return the result of merging the two dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54aee5b5b6c615f629c1fbd6b535e69178df67ae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def agg_child(df, parent_var, df_name):\n",
    "    \"\"\"Aggregate a child dataframe for each observation of the parent.\"\"\"\n",
    "    \n",
    "    # Numeric and then categorical\n",
    "    df_agg = agg_numeric(df, parent_var, df_name)\n",
    "    df_agg_cat = agg_categorical(df, parent_var, df_name)\n",
    "    \n",
    "    # Merge on the parent variable\n",
    "    df_info = df_agg.merge(df_agg_cat, on = parent_var, how = 'outer')\n",
    "    \n",
    "    # Remove any columns with duplicate values\n",
    "    _, idx = np.unique(df_info, axis = 1, return_index = True)\n",
    "    df_info = df_info.iloc[:, idx]\n",
    "    \n",
    "    # memory management\n",
    "    gc.enable()\n",
    "    del df_agg, df_agg_cat\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8d4cfa0b4c67163a58a25cf155b3d22f0dc193f"
   },
   "source": [
    "This function can be applied to both `bureau` and `previous` because these are direct children of `app`. For the children of the children, we will need to take an additional aggregation step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "734cf323c1e22f5baa4f073a720245e28a03a0ef"
   },
   "source": [
    "# Aggregate Grandchild Data Tables\n",
    "\n",
    "Several of the tables (`bureau_balance, cash, credit_card`, and `installments`) are children of the child dataframes. In other words, these are grandchildren of the main `app` data table. To aggregate these tables, they must first be aggregated at the parent level (which is on a per loan basis) and then at the grandparent level (which is on the client basis). For example, in the `bureau_balance` dataframe, there is monthly information on the loans in `bureau`. To get this data into the `app` dataframe will first require grouping the monthly information for each loan and then grouping the loans for each client. \n",
    "\n",
    "Hopefully, the nomenclature does not get too confusing, but here's a rounddown:\n",
    "\n",
    "* __grandchild__: the child of a child data table, for instance, `bureau_balance`. For every row in the child table, there can be multiple rows in the grandchild. \n",
    "* __parent__: the parent table of the grandchild that links the grandchild to the grandparent. For example, the `bureau` dataframe is the parent of the `bureau_balance` dataframe in this situation. `bureau` is in turn the child of the `app` dataframe. `bureau_balance` can be connected to `app` through `bureau`.\n",
    "* __grandparent__: the parent of the parent of the grandchild, in this problem the `app` dataframe. The end goal is to aggregate the information in the grandchild into the grandparent. This will be done in two stages: first at the parent (loan) level and then at the grandparent (client) level\n",
    "* __parent variable__: the variable linking the grandchild to the parent. For the `bureau` and `bureau_balance` data this is `SK_ID_BUREAU` which uniquely identifies each previous loan\n",
    "* __grandparent variable__: the variable linking the parent to the grandparent. This is `SK_ID_CURR` which uniquely identifies each client in `app`.\n",
    "\n",
    "### Aggregating Grandchildren Function\n",
    "\n",
    "We can take the individual steps required for aggregating a grandchild dataframe at the grandparent level in a function. These are:\n",
    "\n",
    "1. Aggregate the numeric variables at the parent (the loan, `SK_ID_BUREAU` or `SK_ID_PREV`) level.\n",
    "2. Merge with the parent of the grandchild to get the grandparent variable in the data (for example `SK_ID_CURR`)\n",
    "3. Aggregate the numeric variables at the grandparent (the client, `SK_ID_CURR`) level. \n",
    "4. Aggregate the categorical variables at the parent level.\n",
    "5. Merge the aggregated data with the parent to get the grandparent variable\n",
    "6. Aggregate the categorical variables at the grandparent level\n",
    "7. Merge the numeric and categorical dataframes on the grandparent varible\n",
    "8. Remove the columns with all duplicated values.\n",
    "9. The resulting dataframe should now have one row for every grandparent (client) observation\n",
    "10. Merge with the main dataframe (`app`) on the grandparent variable (`SK_ID_CURR`). \n",
    "\n",
    "This function can be applied to __all 4 grandchildren__ without the need for hard-coding in specific variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1cebb65093e6937825eb5e99ffe4b1b395203d02",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agg_grandchild(df, parent_df, parent_var, grandparent_var, df_name):\n",
    "    \"\"\"\n",
    "    Aggregate a grandchild dataframe at the grandparent level.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df : dataframe\n",
    "            Data with each row representing one observation\n",
    "            \n",
    "        parent_df : dataframe\n",
    "            Parent table of df that must have the parent_var and \n",
    "            the grandparent_var. Used only to get the grandparent_var into\n",
    "            the dataframe after aggregations\n",
    "            \n",
    "        parent_var : string\n",
    "            Variable representing each unique observation in the parent.\n",
    "            For example, `SK_ID_BUREAU` or `SK_ID_PREV`\n",
    "            \n",
    "        grandparent_var : string\n",
    "            Variable representing each unique observation in the grandparent.\n",
    "            For example, `SK_ID_CURR`. \n",
    "            \n",
    "        df_name : string\n",
    "            String for renaming the resulting columns.\n",
    "            The columns are name with the `df_name` and with the \n",
    "            statistic calculated in the column\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "        df_info : dataframe\n",
    "            A dataframe with one row for each observation of the grandparent variable.\n",
    "            The grandparent variable forms the index, and the resulting dataframe\n",
    "            can be merged with the grandparent to be used for training/testing. \n",
    "            Columns with all duplicate values are removed from the dataframe before returning.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # set the parent_var as the index of the parent_df for faster merges\n",
    "    parent_df = parent_df[[parent_var, grandparent_var]].copy().set_index(parent_var)\n",
    "    \n",
    "    # Aggregate the numeric variables at the parent level\n",
    "    df_agg = agg_numeric(df, parent_var, '%s_LOAN' % df_name)\n",
    "    \n",
    "    # Merge to get the grandparent variable in the data\n",
    "    df_agg = df_agg.merge(parent_df, \n",
    "                          on = parent_var, how = 'left')\n",
    "    \n",
    "    # Aggregate the numeric variables at the grandparent level\n",
    "    df_agg_client = agg_numeric(df_agg, grandparent_var, '%s_CLIENT' % df_name)\n",
    "    \n",
    "    # Can only apply one-hot encoding to categorical variables\n",
    "    if any(df.dtypes == 'object'):\n",
    "    \n",
    "        # Aggregate the categorical variables at the parent level\n",
    "        df_agg_cat = agg_categorical(df, parent_var, '%s_LOAN' % df_name)\n",
    "        df_agg_cat = df_agg_cat.merge(parent_df,\n",
    "                                      on = parent_var, how = 'left')\n",
    "\n",
    "        # Aggregate the categorical variables at the grandparent level\n",
    "        df_agg_cat_client = agg_numeric(df_agg_cat, grandparent_var, '%s_CLIENT' % df_name)\n",
    "        df_info = df_agg_client.merge(df_agg_cat_client, on = grandparent_var, how = 'outer')\n",
    "        \n",
    "        gc.enable()\n",
    "        del df_agg, df_agg_client, df_agg_cat, df_agg_cat_client\n",
    "        gc.collect()\n",
    "    \n",
    "    # If there are no categorical variables, then we only need the numeric aggregations\n",
    "    else:\n",
    "        df_info = df_agg_client.copy()\n",
    "    \n",
    "        gc.enable()\n",
    "        del df_agg, df_agg_client\n",
    "        gc.collect()\n",
    "    \n",
    "    # Drop the columns with all duplicated values\n",
    "    _, idx = np.unique(df_info, axis = 1, return_index=True)\n",
    "    df_info = df_info.iloc[:, idx]\n",
    "    \n",
    "    return df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1139fff129bb081dda6eab4a4b987d8cd9b278d0"
   },
   "source": [
    "# Putting it Together\n",
    "\n",
    "Now that we have the individual pieces of semi-automated feature engineering, we need to put them together. There are two functions that can handle the children and the grandchildren data tables:\n",
    "\n",
    "1. `agg_child(df, parent_var, df_name)`: aggregate the numeric and categorical variables of a child dataframe at the parent level. For example, the `previous` dataframe is a child of the `app` dataframe that must be aggregated for each client. \n",
    "2. `agg_grandchild(df, parent_df, parent_var, grandparent_var, df_name)`: aggregate the numeric and categorical variables of a grandchild dataframe at the grandparent level. For example, the `bureau_balance` dataframe is the grandchild of the `app` dataframe with `bureau` as the parent. \n",
    "\n",
    "For each of the children dataframes of `app`, (`previous` and `bureau`), we will use the first function and merge the result into the `app` on the parent variable, `SK_ID_CURR`. For the four grandchild dataframes, we will use the second function, which returns a single dataframe that can then be merged into app on `SK_ID_CURR`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bf4d830a18b8f7407e1574f81390c81aba5c8be"
   },
   "source": [
    "## Hand-Built Features\n",
    "\n",
    "Along the way, we will add in hand-built features to the datasets. These have come from my own ideas (probably not very optimal) and from the community.\n",
    "\n",
    "First we will add in \"domain knowledge\" features to the `app` dataframe. These were developed based on work done in other kernels (both from the community and my own work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61bd024933e9318fd59e77df40874bfaee83e2fa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add domain features to base dataframe\n",
    "app['LOAN_RATE'] = app['AMT_ANNUITY'] / app['AMT_CREDIT'] \n",
    "app['CREDIT_INCOME_RATIO'] = app['AMT_CREDIT'] / app['AMT_INCOME_TOTAL']\n",
    "app['EMPLOYED_BIRTH_RATIO'] = app['DAYS_EMPLOYED'] / app['DAYS_BIRTH']\n",
    "app['EXT_SOURCE_SUM'] = app[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].sum(axis = 1)\n",
    "app['EXT_SOURCE_MEAN'] = app[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis = 1)\n",
    "app['AMT_REQ_SUM'] = app[[x for x in app.columns if 'AMT_REQ_' in x]].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0b45a58bba944d0e81ba5b12cd18a0e3f87fd84"
   },
   "source": [
    "### Hand-Built Features for other Dataframes\n",
    "\n",
    "We can also add in hand built features for the other dataframes. Since these are not the main dataframe, these features will end up being aggregated in different ways. These will be added as we go through the tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7144bf4ba6ff73fc88cf0e598212db49d8950a56"
   },
   "source": [
    "#### Aggregate the bureau data\n",
    "\n",
    "First add the loan rate for previous loans at other institutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7dc86240f1043ad2e0aa6c1525a2161f373de8d4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bureau['LOAN_RATE'] = bureau['AMT_ANNUITY'] / bureau['AMT_CREDIT_SUM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3791f2c7b78f7b92e545ef5e752a92819efac8c5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bureau_info = agg_child(bureau, 'SK_ID_CURR', 'BUREAU')\n",
    "bureau_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "23f89c917b161b5ed8e4c742e6cd1f087b7f540f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bureau_info.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6595cbe7b551e8473faf934f3d87d2e38dc1afea"
   },
   "source": [
    "#### Aggregate the bureau balance\n",
    "\n",
    "Now we turn to the `bureau_balance` dataframe. We will make a column indicating whether a loan was past due for the month or whether the payment was on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6fbc3d307e40a1c7449c44964170009d303895e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bureau_balance['PAST_DUE'] = bureau_balance['STATUS'].isin(['1', '2', '3', '4', '5'])\n",
    "bureau_balance['ON_TIME'] = bureau_balance['STATUS'] == '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c1237cece65f7db963b2b5d5ab33afe38ec04cb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bureau_balance_info = agg_grandchild(bureau_balance, bureau, 'SK_ID_BUREAU', 'SK_ID_CURR', 'BB')\n",
    "del bureau_balance, bureau\n",
    "bureau_balance_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "093efb9f7bab68e65a480d405fbd3a5d7c259b37",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bureau_balance_info.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d913f7aec61e6e620c0a7a0659800f485a0cb39e"
   },
   "source": [
    "## Merge with the main dataframe\n",
    "\n",
    "The individual dataframes can all be merged into the main `app` dataframe. Merging is much quicker if done on any index, so it's good practice to first set the index to the variable on which we will merge. In each case, we use a `left` join so that all the observations in `app` are kept even if they are not present in the other dataframes (which occurs because not every client has previous records at Home Bureau or other credit institutions). After each step of mergning, we remove the dataframe from memory in order to hopefully let the kernel continue to run.\n",
    "\n",
    "The final result is one dataframe with a single row for each client that can be used for training a machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad82e75a9d61b5a98d28185ff240c3fbbdcd188c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app = app.set_index('SK_ID_CURR')\n",
    "app = app.merge(bureau_info, on = 'SK_ID_CURR', how = 'left')\n",
    "del bureau_info\n",
    "app.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a9e9c7427bd6b80a941ca5c56bb4ef48d17c6f1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app = app.merge(bureau_balance_info, on = 'SK_ID_CURR', how = 'left')\n",
    "del bureau_balance_info\n",
    "app.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca7bea55407fe95859939c54eb857eb69f50949d"
   },
   "source": [
    "#### Aggregate previous loans at Home Credit\n",
    "\n",
    "We will add in two domain features, first the loan rate and then the difference between the amount applied for and the amount awarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13fb6c70040535bfe10f3f56d3a4feee3054bef3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "previous = pd.read_csv('../input/previous_application.csv').replace({365243: np.nan})\n",
    "previous['LOAN_RATE'] = previous['AMT_ANNUITY'] / previous['AMT_CREDIT']\n",
    "previous[\"AMT_DIFFERENCE\"] = previous['AMT_CREDIT'] - previous['AMT_APPLICATION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d107be6e3cf635d0846886bfa97da497915a45a8"
   },
   "source": [
    "`AMT_DIFFERENCE` is the difference between what was given to the client and what the client requested on previous loans at Home Credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f967df7bba80f8fd0f8fce318cd30a6dc746a0b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "previous_info = agg_child(previous, 'SK_ID_CURR', 'PREVIOUS')\n",
    "previous_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46eede109fab05a7665392c2d57c25c45866ed48",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app = app.merge(previous_info, on = 'SK_ID_CURR', how = 'left')\n",
    "del previous_info\n",
    "app.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca236129c93ce2cea3d859bee1fc3f985bd0f8d3"
   },
   "source": [
    "#### Aggregate Installments Data\n",
    "\n",
    "The installments table has each installment (payment) for previous loans at Home Credit. We can create a column indicating whether or not a loan was late."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a49ddecbc20ccc1deaede33a6921ebe92f29c21e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "installments = pd.read_csv('../input/installments_payments.csv').replace({365243: np.nan})\n",
    "\n",
    "installments['LATE'] = installments['DAYS_ENTRY_PAYMENT'] > installments['DAYS_INSTALMENT']\n",
    "installments['LOW_PAYMENT'] = installments['AMT_PAYMENT'] < installments['AMT_INSTALMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c5a277729fdd8800f9a76ab6c1498f40abde821"
   },
   "source": [
    "`LOW_PAYMENT` represents a payment that was less than the prescribed amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec76d1aadb13750f75352eb65f852fc0adb32b55",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "installments_info = agg_grandchild(installments, previous, 'SK_ID_PREV', 'SK_ID_CURR', 'IN')\n",
    "del installments\n",
    "installments_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "129b59883521f0dd2ea4789612540b041422a6bc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app = app.merge(installments_info, on = 'SK_ID_CURR', how = 'left')\n",
    "del installments_info\n",
    "app.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90d1f7e93b4d93cc014ee01ac600b8ff383e695b"
   },
   "source": [
    "#### Aggregate Cash previous loans\n",
    "\n",
    "The next dataframe is the `cash` which has monthly information on previous cash loans at Home Credit. We can create a column indicating if the loan was overdue for the month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f777ed7ad8cdeb34b80f1f99242a2947ba3ac67",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cash = pd.read_csv('../input/POS_CASH_balance.csv').replace({365243: np.nan})\n",
    "\n",
    "cash['LATE_PAYMENT'] = cash['SK_DPD'] > 0.0\n",
    "cash['INSTALLMENTS_PAID'] = cash['CNT_INSTALMENT'] - cash['CNT_INSTALMENT_FUTURE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd946d6dd97d170ab0e22904dda4e6eafde4e944"
   },
   "source": [
    "`INSTALLMENTS_PAID` is meant to represent the number of already paid (or I guess missed) installments by subtracting the future installments from the total installments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "11ef3be67752e60cbeaad78b3da521889253d408",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cash_info = agg_grandchild(cash, previous, 'SK_ID_PREV', 'SK_ID_CURR', 'CASH')\n",
    "del cash\n",
    "cash_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "690bbf472ea0b4934e0589c45caeb6590f073724",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app = app.merge(cash_info, on = 'SK_ID_CURR', how = 'left')\n",
    "del cash_info\n",
    "app.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0dd5fbe21c4bdcc6f04d3b2d094f38e1ad8ceb8f"
   },
   "source": [
    "#### Aggregate Credit previous loans\n",
    "\n",
    "The last dataframe is `credit` which has previous credit card loans at Home Credit. We can make a column indicating whether the balance is greater than the credit limit, a column showing whether or not the balance was cleared (equal to 0), whether or not the payment was below the prescribed amount, and whether or not the payment was behind. Then we aggregate as with the other grandchildren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dc16ee25b0acbb3042179cc80e17fda23e0ecf4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credit = pd.read_csv('../input/credit_card_balance.csv').replace({365243: np.nan})\n",
    "\n",
    "credit['OVER_LIMIT'] = credit['AMT_BALANCE'] > credit['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "credit['BALANCE_CLEARED'] = credit['AMT_BALANCE'] == 0.0\n",
    "credit['LOW_PAYMENT'] = credit['AMT_PAYMENT_CURRENT'] < credit['AMT_INST_MIN_REGULARITY']\n",
    "credit['LATE'] = credit['SK_DPD'] > 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "854c362609f29368606d6a4459374cdb02ece9bc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credit_info = agg_grandchild(credit, previous, 'SK_ID_PREV', 'SK_ID_CURR', 'CC')\n",
    "del credit, previous\n",
    "credit_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4bcf80e7ff66a8e995cb770bbf6e021b45133757",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c89e4da8a4719b2e1c0dbbb7f8be507939ed1db5"
   },
   "source": [
    "__This is usually the point at which the kernel fails.__ To try and alleviate the problem, I have added a pause of 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eee572c759717fa9629cb4a91eea9468d00cf3e8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(600)\n",
    "app = app.merge(credit_info, on = 'SK_ID_CURR', how = 'left')\n",
    "del credit_info\n",
    "app.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9bcd815bb39284de46390de086da3aed84f1b598",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('After manual feature engineering, there are {} features.'.format(app.shape[1] - 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a0b0a1a8f48e9019fa849f6131f5a5b97ed8333",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c5bc4b21856f62faf015dd9ba93b752800efb8e"
   },
   "source": [
    "__Update July 25__: Nothing past this point is able to run so I've commented it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef97710d56b1c58ac1d5332269836136b8b36f07",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Check for columns with duplicated values\n",
    "# _, idx = np.unique(app, axis = 1, return_index = True)\n",
    "# print('There are {} columns with all duplicated values.'.format(app.shape[1] - len(idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8af0cb98f65449572d40c7d678a534c7df1c8e63"
   },
   "source": [
    "# Modeling\n",
    "\n",
    "After all the hard work, now we get to test our features! We will use a model with the hyperparameters from random search that are documented in another notebook. \n",
    "\n",
    "The final model scores __0.792__ when uploaded to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66f7f7b37549747b30030eaa8b2e7a70ee161b4a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train, test = app[app['TARGET'].notnull()].copy(), app[app['TARGET'].isnull()].copy()\n",
    "# gc.enable()\n",
    "# del app\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7feb3e3029f28812bc166ba80c2128a1af1e6ab5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "\n",
    "# random_hyp = {'is_unbalance': True, \n",
    "#               'n_estimators': 2673, \n",
    "#               'num_leaves': 77, \n",
    "#               'learning_rate': 0.00764, \n",
    "#               'min_child_samples': 460, \n",
    "#               'boosting_type': 'gbdt', \n",
    "#               'subsample_for_bin': 240000, \n",
    "#               'reg_lambda': 0.20, \n",
    "#               'reg_alpha': 0.88, \n",
    "#               'subsample': 0.95, \n",
    "#               'colsample_bytree': 0.7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6bc6a2b44da845ddab8b7f88395112d290844f17",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_labels = np.array(train.pop('TARGET')).reshape((-1, ))\n",
    "\n",
    "# test_ids = list(test.pop('SK_ID_CURR'))\n",
    "# test = test.drop(columns = ['SK_ID_CURR'])\n",
    "\n",
    "# print('Training shape: ', app.shape)\n",
    "# print('Testing shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61ebfbe2de9bdf188ca270b50dd0c28316e855b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = lgb.LGBMClassifier(**random_hyp)\n",
    "# model.fit(app, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6add38b6fa4652158c47f63e8fdf207037ab3d9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preds = model.predict_proba(test)[:, 1]\n",
    "# submission = pd.DataFrame({'SK_ID_CURR': test_ids,\n",
    "#                            'TARGET': preds})\n",
    "# submission.to_csv('submission_manual.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eddaafba40d5cc85a6c549200738050cd3f7c699"
   },
   "source": [
    "## Feature Importances\n",
    "\n",
    "Now we can see if all that time was worth it! In the code below, we find the most important features and show them in a plot and dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3fb3f9a6f83a74cf410a93e51fcd382c84b6e680",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features = list(train.columns)\n",
    "# fi = pd.DataFrame({'feature': features,\n",
    "#                    'importance': model.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7addb0e674080e3559a253b4dc3cbc574ff8b35",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def plot_feature_importances(df, n = 15, threshold = None):\n",
    "#     \"\"\"\n",
    "#     Plots n most important features. Also plots the cumulative importance if\n",
    "#     threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n",
    "#     Intended for use with any tree-based feature importances. \n",
    "    \n",
    "#     Parameters\n",
    "#     --------\n",
    "#     df : dataframe\n",
    "#         Dataframe of feature importances. Columns must be \"feature\" and \"importance\"\n",
    "    \n",
    "#     n : int, default = 15\n",
    "#         Number of most important features to plot\n",
    "    \n",
    "#     threshold : float, default = None\n",
    "#         Threshold for cumulative importance plot. If not provided, no plot is made\n",
    "        \n",
    "#     Return\n",
    "#     --------\n",
    "#     df : dataframe\n",
    "#         Dataframe ordered by feature importances with a normalized column (sums to 1)\n",
    "#         and a cumulative importance column\n",
    "    \n",
    "#     Note\n",
    "#     --------\n",
    "#         * Normalization in this case means sums to 1. \n",
    "#         * Cumulative importance is calculated by summing features from most to least important\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Sort features according to importance\n",
    "#     df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "#     # Normalize the feature importances to add up to one\n",
    "#     df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "#     df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "    \n",
    "#     plt.rcParams['font.size'] = 12\n",
    "    \n",
    "#     # Bar plot of n most important features\n",
    "#     df.loc[:n, :].plot.barh(y = 'importance_normalized', \n",
    "#                             x = 'feature', color = 'blue', edgecolor = 'k', figsize = (12, 8),\n",
    "#                             legend = False)\n",
    "\n",
    "#     plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n",
    "#     plt.title(f'Top {n} Most Important Features', size = 18)\n",
    "#     plt.gca().invert_yaxis()\n",
    "    \n",
    "#     if threshold:\n",
    "#         # Cumulative importance plot\n",
    "#         plt.figure(figsize = (8, 6))\n",
    "#         plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n",
    "#         plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n",
    "#         plt.title('Cumulative Feature Importance', size = 18);\n",
    "        \n",
    "#         # Number of features needed for threshold cumulative importance\n",
    "#         importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
    "        \n",
    "#         # Add vertical line to plot\n",
    "#         plt.vlines(importance_index + 1, ymin = 0, ymax = 1.2, linestyles = '--', colors = 'red')\n",
    "#         plt.show();\n",
    "        \n",
    "#         print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, 100 * threshold))\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f753460123fbac52ff0b7a17af3ecb1bda2a2a61",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# norm_fi = plot_feature_importancesortances(fi, 25)\n",
    "# norm_fi.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8a7f98ac270a6d8e1edf2f7ee32ab1122a8692e"
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "This code is a little too much to run in the Kaggle kernels. However, the features themselves are available at https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features under `clean_manual.csv`. The feature importances for these features in a gradient boosting model are also available at the same link with the name `fi_clean_manual.csv`. \n",
    "\n",
    "This notebook is meant to serve as a clean version of the manual feature engineering I had scattered across several other notebooks. We were able to build a complete set of __ features that scored 0.792 on the public leaderboard__. Further hyperparameter tuning might improve the performance. For additional feature engineering, we will probably want to turn to more technical operations such as treating this as a time-series problem. Since we have relative time information (relative to the current loan at Home Credit), it's possible to find the most recent information and also trends over time. These can be useful because changes in behavior might inform us as to whether or not a client will be able to repay a loan! \n",
    "\n",
    "Thanks for reading and as always, I welcome feedback and constructive criticism. I'll see you in the next notebook.\n",
    "\n",
    "Best,\n",
    "\n",
    "Will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab8c60ef7fddf53afdca48e0c1ed15a665b82c8b",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
